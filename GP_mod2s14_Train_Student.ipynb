{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS4GZ37Wgcjr"
      },
      "source": [
        "*Unit 4, Sprint 2, Module 2*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etFf1WLWgcjt",
        "toc-hr-collapsed": false
      },
      "source": [
        "# Train (Prepare)\n",
        "__*Neural Network Foundations*__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXB80QOhgcju"
      },
      "source": [
        "## Learning Objectives\n",
        "* <a href=\"#p1\">Part 1</a>: Student should be able to explain the intuition behind backpropagation and gradient descent\n",
        "* <a href=\"#p2\">Part 2</a>: Student should be able to discuss the importance of the batch size hyperparameter\n",
        "* <a href=\"#p3\">Part 3</a>: Student should be able to discuss the importance of the learning rate hyperparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YuQu2lfgcju"
      },
      "source": [
        "## Summary of Module 1\n",
        "\n",
        "In Module 1, we learned about Neural Networks and related concepts: Neurons, Weights, Activation Functions, and Layers (input, output, & hidden). Today, we will reinforce our understanding, and learn how a neural network is trained. **Feed-forward neural networks**, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of **gradient descent** where gradients has been calculated by **back-propagation**.\n",
        "\n",
        "  <center><img src=\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/main/module1-Architect/IMG_0167.jpeg\" width=400></center>\n",
        "\n",
        "- There are three kinds of layers: **input**, **hidden**, and **output**\n",
        "- Hidden and output layers are made up of individual neurons (aka activation units) which have a corresponding weight and bias.\n",
        "- Information is passed from layer to layer through a network by:\n",
        " - Taking in inputs from the training data (or previous layer)\n",
        " - Multiplying each input by its corresponding weight (represented by arrows) and adding these products to form a weighted sum\n",
        " - Adding a bias (also represented by arrows)\n",
        " - Passing this weighted sum plus the bias term into an activation function such as sigmoid or relu or some other activation function <br>\n",
        " As an example: for a `perceptron` with three inputs and a sigmoid activation function, the output is calculated as follows: <br><br>\n",
        "\\begin{align}\n",
        " y = \\text{sigmoid} \\left(weight_{1}\\times input_{1} + weight_{2} \\times input_{2} + weight_{3} \\times input_{3} + bias\\right)\n",
        "\\end{align} <br><br>\n",
        " - This \"activated\" neuron output $y$ is the signal that gets passed into the next layer of the network\n",
        " - the \"activated\" neuron output of the output layer is the prediction\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpi4R03rgcjv"
      },
      "source": [
        "## Training a Neural Network for Classification: *Formal Summary*\n",
        "\n",
        "1. Choose a network architecture\n",
        "   - Number of input units = Number of features\n",
        "   - Number of output units = Number of Classes (or categories of expected targets)\n",
        "   - Select the number of hidden layers and the number of neurons within each hidden layer\n",
        "   - set the activation functions for each neuron\n",
        "2. Randomly initialize all the weights and biases\n",
        "3. Forward propagate the weights and biases through the network to compute the output predictions $\\hat y$\n",
        "4. Given the output predictions  $\\hat y$ and the true targets  $y$, compute the loss (cost) function $J(\\theta)$, where $\\theta$ is the set of values of all the weights and biases in the network.\n",
        "5. Perform **back-propagation**, which means computing partial derivatives (gradients) of the cost function with respect to the each of the weights and biases $\\frac{\\partial}{\\partial\\theta}{J(\\theta)}$\n",
        "6. Using the method of **gradient descent** (or other advanced optimizer), adjust the weights and biases so as to decrease the cost function $J(\\theta)$.\n",
        "7. Repeat steps 3 - 6 until the cost function is 'minimized' or some other stopping criteria is met. One pass over steps 3 - 6 is called an iteration.\n",
        "\n",
        "### Highly recommended: Check out this [Neural Networks mini-course from 3Blue1Brown](https://www.3blue1brown.com/topics/neural-networks)<br>\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM4CK1IarId4",
        "toc-hr-collapsed": false
      },
      "source": [
        "------\n",
        "# 1. Backpropagation & Gradient Descent (Learn)\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktm8Fmoagcjy",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Overview\n",
        "\n",
        "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific algorithm using calculus to update the weights and biases in a neural network in reverse order, at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, watch [Backpropagation calculus ](https://www.youtube.com/watch?v=tIeHLnjs5U8) from 3Blue1Brown. Also highly recommended is this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walk through of the main intuition and math behind the backpropagation algorithm. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXI2tEO9gcjy"
      },
      "source": [
        "## 1.1 What is a Gradient?\n",
        "\n",
        "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
        "\n",
        "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7SvE-6H9G4y"
      },
      "source": [
        "### 1.1.1 Review: Slope of a line \n",
        "\n",
        "![](https://www.mathwarehouse.com/algebra/linear_equation/images/slope-of-a-line/slope-formula-all_no_highlight.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGYuD9yI9G4z"
      },
      "source": [
        "### 1.1.2 Review: A basic derivative \n",
        "The derivative of a one-dimensional function at any point is defined as the slope of the line tangent to the function at that point.\n",
        "\n",
        "![](https://ginsyblog.files.wordpress.com/2017/02/derivativelimitdef.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFs3TG5O9G4z"
      },
      "source": [
        "### 1.1.3 Gradients are multi-dimensional derivatives\n",
        "\n",
        "Each partial derivative can be considered as the component of a vector that lies on the axis of the associated parameter.  <br>\n",
        "Formally, the gradient is the vector sum of these partial derivative vectors.\n",
        "\n",
        "*The gradient of a function at a given point contains information about the magnitude <br>\n",
        "and direction of the sensitivity of the function to change in every parameter.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVN-8elU9G40"
      },
      "source": [
        "##1.2. Gradient Descent Formula for updating neural network parameters\n",
        "\n",
        "Given the values of the current parameters (weights and biases), this formula prescribes how to update them in order to drive down the cost function, given their current values:\n",
        "\n",
        "$$ \\huge{\\theta_j = \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}}$$\n",
        "\n",
        "\n",
        "In this formula, \n",
        "- $\\Large{\\theta_j}$ refers to the vector of weights and biases for the $jth$ data point<br><br>\n",
        "- $\\Large{\\theta}$ refers to the combined set of weights and biases of all the data points<br><br>\n",
        "- $\\Large{J(\\theta)}$ represents the cost function, which is a measure of the error between the predicted target values and the true target values<br><br>\n",
        "- $\\Large{\\alpha}$ is the *learning rate*, a positive scalar hyper-parameter<br><br>\n",
        "- $\\Large{\\frac{\\partial J(\\theta)}{\\partial \\theta_j}}$ are the partial derivatives of the cost function, which measure its sensitivity to changes in the parameters $\\Large{\\theta_j}$<br><br>\n",
        "\n",
        "\n",
        "More sophisticated versions of gradient descent are discussed in the excellent article [**An overview of gradient descent optimization algorithms**](https://ruder.io/optimizing-gradient-descent/) by Sebastian Ruder.\n",
        "\n",
        "\n",
        "These versions all build on the Gradient Descent Formula presented above, so it's best to understand this before moving on to more sophisticated versions. <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQc8iy_u9G40"
      },
      "source": [
        "### 1.2.1 Geometry of Gradient Descent \n",
        "in one dimension\n",
        "\n",
        "![](https://i.stack.imgur.com/yk1mk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlPoxfuS9G41"
      },
      "source": [
        "### 1.2.2 Convex vs. Non-Convex Loss Functions\n",
        "\n",
        "Loss curves won't always have a single minimum point for gradient descent to converge towards. Sometimes there are multiple minimums. \n",
        "\n",
        "**Global Minimum:** The absolute minimum location of a curve (or surface). \n",
        "\n",
        "**Local Minimum:** The local minimum location of a curve (or surface). \n",
        "\n",
        "\n",
        "### In 3-Dimensions \n",
        "In Calculus, those ambiguous points that take on the form of both local minima and local maxima are known as [**saddle points**](https://en.wikipedia.org/wiki/Saddle_point). It's not necessary to dive into the mathematics, the key take away is that non-convex error curves (and surfaces) have this global/local minimum issue. \n",
        "\n",
        "![](https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/convex-non-convex-9c8cb9320d4b0392c5f67004e8832e85.jpg)\n",
        "\n",
        "\n",
        "**Take Away:** Sometimes the gradient descent algorithm converges but is actually trapped in a local minimum. \n",
        "\n",
        "There are at least 2 possible solutions to this problem: \n",
        "\n",
        "1) Try different approaches to randomly initalizing the model weights\n",
        "For this check out [Keras's docs on Weight Initializers](https://keras.io/api/layers/initializers/). Treat these weight initializers as just another hyper-parameter to include in your gridsearch. It's a good idea to get into the practice of including weight initializers in your gridsearches! \n",
        "\n",
        "\n",
        "2) Try non-gradient descent optimizers such as [Particle Swarm](https://en.wikipedia.org/wiki/Particle_swarm_optimization) or [Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm). Feel free to read up on these approaches but know that **you are not expected to know these approaches** and they are outside the scope of this course. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZY66kiUgcjz",
        "toc-hr-collapsed": true
      },
      "source": [
        "------\n",
        "## 1.3 Let's see how to implement Gradient Descent in Keras for a simple toy problem! (Follow Along)\n",
        "\n",
        "In this section, we will again build a simple neural network using base TensorFlow. <br>\n",
        "We'll train a __Feed Forward Neural Network__ to predict targets,<br> using the method of __Gradient Descent__ to adjust the neural network parameters.<br><br> This is the process of __Back-propagation__!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d4tzpwO6B47"
      },
      "source": [
        "### 1.3.1 Generate synthetic (fake) data for a simple linear regression problem<br>\n",
        "$y = \\textbf{w} \\cdot \\textbf{x} + b + \\text{noise}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERyVgeO_IWyV"
      },
      "source": [
        "# plotting \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# dataset import\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# tensorflow imports for building \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-13ede96854baf6e5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_C6O65Q09G43"
      },
      "source": [
        "# Specify the model parameters: slope(weight) and intercept (bias)\n",
        "w_true = 3.5\n",
        "b_true = 50.0\n",
        "# Specify how many examples we want to generate\n",
        "n_examples = 1000\n",
        "\n",
        "# linear regression assumes normally distributed noise, \n",
        "#    so we'll draw noise values from Gaussian distributions \n",
        "noise = tf.random.normal(mean=0.0, stddev=1.0, shape=[n_examples])\n",
        "\n",
        "# we'll draw input values from a uniform distribution\n",
        "x = tf.random.uniform(minval=-1, maxval=1, shape=[n_examples])\n",
        "\n",
        "# Generate samples from the Linear Regression data model\n",
        "# y = w*x + b + noise\n",
        "y_true = w_true * x + b_true + noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJesGEUgcj4"
      },
      "source": [
        "### 1.3.2 Loss Function\n",
        "The Mean Squared Error (MSE), is an appropriate loss function for a regression problem. We are trying to predict a continuous target.\n",
        "\n",
        "$$\\huge{\\text{MSE} = \\frac{1}{N}\\sum_{i=1}^{N} (y_i-\\hat{y}_i)^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyITYhMS9G44"
      },
      "source": [
        "def loss(target_y, predicted_y, model, reg_strength=0.0):\n",
        "    \"\"\"\n",
        "    Implements Mean Square Error (MSE) as the loss function\n",
        "    \"\"\"\n",
        "    return tf.reduce_mean(tf.square(target_y - predicted_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgTf6vTS69Sw"
      },
      "source": [
        "### 1.3.3 Neural Network Architecture\n",
        "Lets create a Neural Network class called `Model` to feed forward the input to a neural network composed of a single linear layer, with no activation function. Note: This is essentially a linear regression model whose coefficients are trained by gradient descent. In practice, gradient descent works on much more complex functions like the multi-layer networks we constructed in Module 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUI8VSR5zyBv"
      },
      "source": [
        "class Model(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.w = tf.Variable(8.0)\n",
        "        self.b = tf.Variable(40.0)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # returns w*x + b \n",
        "        return self.w * x + self.b\n",
        "\n",
        "# Instantiate the Model() class\n",
        "model = Model()\n",
        "\n",
        "# test the Model() class\n",
        "#    with parameters w = 8 and b = 40, our model is  y(x) = w*x + b = 8\n",
        "#    with input of x = 3, we expect y = 8*3 + 40 = 64\n",
        "\n",
        "assert model(3.0).numpy() == 64.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbyT_FJ88IlK"
      },
      "source": [
        "### 1.3.4 Prediction with Initial Weights\n",
        "The weights in our model were initialized randomly, so of course our neural network's initial predictions are way off!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreIDe6P8H0H"
      },
      "source": [
        "inputs = x\n",
        "outputs = y_true\n",
        "predictions = model(inputs)\n",
        "plt.scatter(inputs, outputs, c='b', label = \"data\")\n",
        "plt.plot(inputs, predictions, c='r', label = \"model predictions\")\n",
        "plt.xlabel('x',FontSize=14)\n",
        "plt.ylabel('y',FontSize=14)\n",
        "plt.legend()\n",
        "plt.grid();\n",
        "\n",
        "print('Current loss: %1.6f' % loss(predictions, outputs, model).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ujj6vNYQyX",
        "toc-hr-collapsed": true
      },
      "source": [
        "### 1.3.5 Back-propagation\n",
        "Means iteratively updating the weights and bias based on the gradients of the loss function.<br>\n",
        "We'll write a helper function to train a model using Keras!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgaGD6YlHoid",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-100d1b1df12abe63",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        " # this train function updates w and b using partial derivative of the loss function w.r.t. w and b\n",
        " def train(model, inputs, outputs, learning_rate):\n",
        "    with tf.GradientTape() as tape: \n",
        "                  \n",
        "      # calculate the loss function value from our model's predictions\n",
        "      current_loss = loss(outputs, model(inputs), model)\n",
        "          \n",
        "      # calculate dw and db, the gradients of the loss function with respect to w and b \n",
        "      dw, db = tape.gradient(current_loss, [model.w, model.b]) \n",
        "      \n",
        "      # update the value of w by subtracting (learning rate) * (gradient of the loss function wrt w) from its current value\n",
        "      model.w.assign_sub(learning_rate * dw)\n",
        "      \n",
        "      # update the value of b by subtracting (learning rate) * (gradient of the loss function wrt b) from its current value\n",
        "      model.b.assign_sub(learning_rate * db)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iziWWURgck8"
      },
      "source": [
        "### 1.3.6 Train the Network\n",
        "At each epoch, we'll print the values of the updated parameters and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zn_HgFuHhTr"
      },
      "source": [
        "model = Model()\n",
        "\n",
        "# Store parameter history\n",
        "ws, bs = [], []\n",
        "current_loss = []\n",
        "epochs = range(50)\n",
        "\n",
        "for epoch in epochs:\n",
        "\n",
        "    # forward propagate the current parameters through the network to predict targets\n",
        "    y_hat =  model(inputs)\n",
        "\n",
        "    # compute the loss function by comparing target predictions to the true targets\n",
        "    current_loss.append(loss(outputs, y_hat, model))\n",
        "\n",
        "    # update the parameters (weight and bias) based on gradient descent\n",
        "    train(model, inputs, outputs, learning_rate=0.1)\n",
        "\n",
        "    # store the updated values of weight and bias\n",
        "    ws.append(model.w.numpy())\n",
        "    bs.append(model.b.numpy())\n",
        "\n",
        "    print('Epoch %2d: w=%1.2f b=%1.2f loss=%2.5f' % (epoch, ws[-1], bs[-1], current_loss[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSEt07wdHvi2"
      },
      "source": [
        "plt.plot(epochs, ws, 'r', epochs, bs, 'b')\n",
        "plt.plot([w_true] * len(epochs), 'r--',\n",
        "         [b_true] * len(epochs), 'b--')\n",
        "plt.xlabel('Epoch number',FontSize=14)\n",
        "plt.ylabel('Parameter value',FontSize=14)\n",
        "plt.legend(['w', 'b', 'True w', 'True b'])\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cmu3mtNTVDf"
      },
      "source": [
        "plt.plot(epochs, current_loss, 'k')\n",
        "plt.xlabel('Epoch number',FontSize=14)\n",
        "plt.ylabel('Loss',FontSize=14)\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pKDfpplbUxN"
      },
      "source": [
        "plt.scatter(inputs, outputs, c='b', label = \"data\")\n",
        "plt.plot(inputs, model(inputs),c='r',label = \"model predictions\")\n",
        "plt.legend()\n",
        "plt.xlabel('x',FontSize=14)\n",
        "plt.ylabel('y',FontSize=14)\n",
        "plt.grid()\n",
        "\n",
        "print('Current loss: %1.6f'% current_loss[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N4lWxfS6YIQ"
      },
      "source": [
        "### Nice job! Our model closely fits the data. \n",
        "### Congratulations! <br>\n",
        "You've just learned how to train a neural network using gradient descent and back-propagation with Keras!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKUVGoRxgck_"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "In the module project, you will be asked to explain the logic of back-propagation and gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFGB1EXNwxM-"
      },
      "source": [
        "## Investigate the effects of the `batch_size` and `learning_rate` hyperparameters.\n",
        "\n",
        "In parts 2 and 3, we'll return to modeling the MNIST handwritten digit data set. <br>\n",
        "Although we'll adjust `batch_size` and `learning_rate` separately, they are in fact related. <br>\n",
        "Larger `batch_size` gives more accurate parameter updates, so we can confidently move toward the minimum of the loss function with larger steps, i.e. we can use larger values of `learning_rate`. Conversely, smaller batch sizes give \"noisier\" updates, so we should take smaller steps, i.e. use smaller values of `learning_rate`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTqZg-6igclA",
        "toc-hr-collapsed": true
      },
      "source": [
        "# 2. Effect of the Batch Size Hyperparameter on Training a Neural Network (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nrm-racgclA"
      },
      "source": [
        "## 2.1 Overview\n",
        "\n",
        "### 2.1.1 Stochastic Gradient Descent\n",
        "\n",
        "Stochastic Gradient Descent (SGD) estimates the gradients of the loss function over the entire dataset from the predictions of a **single** randomly chosen sample. With these gradients, we can then update the parameters. \n",
        "\n",
        "Why use a single sample instead of all of the samples (a batch), or a random subset of the samples (a minibatch)?\n",
        "\n",
        "Because calculating the gradient over the entire dataset (or a subset) is expensive computationally compared to using a single sample.\n",
        "\n",
        "So parameter updates in SGD are super-fast. What's the tradeoff? Can't get something for nothing!<br>\n",
        "The tradeoff is that SGD produces a noisy parameter update, because we are only using a small amount of information (a single sample) to compute the update!\n",
        "\n",
        "**Takeaway**: SGD computes *rapid* but *noisy* parameter updates!\n",
        "\n",
        "### 2.1.2 Minibatch Gradient Descent\n",
        "In minibatch Gradient Descent, the parameters are updated based on a batch of data points. Batch size is the number of data points our model uses to update the parameters and predict target values. A batch is a randomly selected subset of an epoch. All observations are eventually used when processing an epoch.\n",
        "\n",
        "* Smaller Batch = Slower Run Time, but potentially better generalization accuracy (due to noisier updates helping against overfitting!)\n",
        "* Default Batch = Balance between speed and generalization accuracy\n",
        "* Large Batch = Faster run time, but potentially worse generalization accuracy due to overfitting\n",
        "\n",
        "### 2.1.3 Batch Gradient Descent\n",
        "Another way to do Gradient Descent is to use all the data to compute each update, so that the parameters get updated only every epoch. But this is often problematic, because the whole dataset would have to fit in memory, and dataset can be huge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNQ2ZCi7I4i6"
      },
      "source": [
        "## 2.2 Baseline Model with MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjW2lYVI9Q2",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1c90a81f1eece31b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# normalize \n",
        "max_pixel_value = 255\n",
        "X_train = X_train / max_pixel_value\n",
        "X_test = X_test / max_pixel_value\n",
        "\n",
        "# reshape\n",
        "X_train = X_train.reshape((60000, 784))\n",
        "X_test = X_test.reshape((10000, 784))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkvukt_X9G48"
      },
      "source": [
        "### 2.2.1 Normalizing the data can help your model learn faster\n",
        "\n",
        "If your dataset has values ranging across multiple orders of magnitude (i.e. $10^1,~~10^2,~~10^3,~~10^4$), then gradient descent will update the weights and biases in grossly uneven proportions.<br> \n",
        "\n",
        "**Normalizing** the data, i.e shifting and scaling the values to lie within the unit interval $[0,1]$ ensures that all weight and bias updates are on the same scale, which can lead to quicker convergence. \n",
        "\n",
        "![](https://quicktomaster.com/wp-content/uploads/2020/08/contour_plot.png)\n",
        "\n",
        "There's more to be said about Normalization and Gradient Descent <br>\n",
        "If you are interested in going deeper, we highly recommend this well written article [Normalizing your data](https://www.jeremyjordan.me/batch-normalization/) in which Jeremy Jordan explores the impact of normalization on Gradient Descent in greater detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPDPo4LG1gxY"
      },
      "source": [
        "### 2.2.2 Helper function to build our neural network model\n",
        "Since this is a classification problem with 10 classes and the target values are integers the [appropriate loss function](https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other) to use in our model is `sparse_categorical_crossentropy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7x17kDKJSy5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-38ed3365b403af52",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# write a function called create_model that accepts a learning rate alpha for SGD as an input parameter \n",
        "# it should return a complied, 2 hidden layer neural net that uses SGD as the optimizer \n",
        "# Import SGD as discussed here: https://keras.io/api/optimizers/sgd/\n",
        "\n",
        "# create create_model\n",
        "def create_model(lr=.01):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr: float\n",
        "        Learning rate parameter used for Stocastic Gradient Descent \n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "        A complied keras model \n",
        "    \"\"\"\n",
        "    ### BEGIN SOLUTION\n",
        "    \n",
        "\n",
        "    return model\n",
        "    ### END SOLUTION "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-HsAQ-9jgUM"
      },
      "source": [
        "create_model().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7UE-KluPsX"
      },
      "source": [
        "## 2.3 Experiments to gauge the effect of batch size (Follow Along)\n",
        "Let's run a series of experiments for a default, small, and large batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhpDaVFRJl3U"
      },
      "source": [
        "### 2.3.1 Default Batch Size\n",
        "Batch Size is 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-ChVGikgclD",
        "scrolled": true
      },
      "source": [
        "%%time\n",
        "# instantiate a model and fit it with batch size of 32\n",
        "model = create_model()\n",
        "bt_default = model.fit(X_train, y_train, batch_size=32, validation_data=(X_test, y_test), epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krYQ1XHvnpk_"
      },
      "source": [
        "What's that number 1875? It's the number of batches in the full data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knaV6-vBjwNQ"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_train.shape[0]/32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0hxaeqyj4ye"
      },
      "source": [
        "60000/32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvsbOFnDJuG0"
      },
      "source": [
        "### 2.3.2 Small Batch Size\n",
        "Batch Size is 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diDzvb-UJ1je",
        "scrolled": true
      },
      "source": [
        "%%time\n",
        "# instantiate a model and fit it with batch size of 8\n",
        "#YOUR CODE HERE\n",
        "model = create_model()\n",
        "bt_small = model.fit(X_train, y_train, batch_size=8, validation_data=(X_test, y_test), epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iPvvvt5J2Xl"
      },
      "source": [
        "### 2.3.3 Large Batch Size\n",
        "Batch Size is 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h8Z5293KABT"
      },
      "source": [
        "# instantiate a model and fit it with batch size of 512\n",
        "#YOUR CODE HERE\n",
        "model = create_model()\n",
        "bt_large = model.fit(X_train, y_train, batch_size=512, validation_data=(X_test, y_test), epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE2Awnk3aXPv"
      },
      "source": [
        "bt_default.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQFCYnUSakDU"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dd = pd.DataFrame.from_dict(bt_default.history)\n",
        "dd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ujUz6BKUGz"
      },
      "source": [
        "### 2.3.4 Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-5DOZNMKYt-"
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "experiment_dfs = []\n",
        "\n",
        "for experiment, label in zip([bt_default, bt_small, bt_large], [\"32_\", \"8_\", \"512_\"]):\n",
        "\n",
        "    df = pd.DataFrame.from_dict(experiment.history)\n",
        "    df['epoch'] = df.index.values\n",
        "    df['Batch Size'] = label\n",
        "\n",
        "    # append to list of experiment data frames\n",
        "    experiment_dfs.append(df)\n",
        "\n",
        "df = pd.concat(experiment_dfs,ignore_index=True)\n",
        "df['Batch Size'] = df['Batch Size'].astype('str')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlg9uSEEmIJB"
      },
      "source": [
        "sns.lineplot(x='epoch', y='val_accuracy', hue='Batch Size', data=df);\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94bJYgz3nkp0"
      },
      "source": [
        "sns.lineplot(x='epoch', y='val_loss', hue='Batch Size', data=df);\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kZ2vUYYgclS"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to experiment with batch size on today's assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46cP9Pm_gclS"
      },
      "source": [
        "# 3. The Effect of the Learning Rate Hyperparameter on Neural Network Training (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXBoghYG9G5A"
      },
      "source": [
        "![](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bna67ADZgclT",
        "toc-hr-collapsed": true
      },
      "source": [
        "## 3.1 Overview\n",
        "\n",
        "Learning Rate controls the size of the update to our weights that the optimization algorithm makes. VERY IMPORTANT hyperparameter.\n",
        "\n",
        "* If the learning rate is too high, the model will not find the minimum\n",
        "* If the learning rate is too low, the model will underfit or take too long to converge\n",
        "* Goldilocks learning rate is \"just right\", the model converges rapidly\n",
        "* Scale of 0-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsVYOn7bgcle",
        "toc-hr-collapsed": true
      },
      "source": [
        "## 3.2 Experiments to gauge the effect of learning rate (Follow Along)\n",
        "\n",
        "Try experiments with same Batch Size = 32 but different Learning Rates:\n",
        "* High Learning Rate = .75\n",
        "* Default Learning Rate = .01\n",
        "* Low Learning Rate = .0001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI_H8Em1NOii"
      },
      "source": [
        "### 3.2.1 Default Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se8cb_ZUNVtL"
      },
      "source": [
        "# instantiate a model and fit it with a learning rate value of 0.01\n",
        "model = create_model(lr=.01)\n",
        "lr_default = model.fit(X_train, y_train, batch_size=32, validation_data=(X_test, y_test), epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQZ4SZdKNMRO"
      },
      "source": [
        "### 3.2.2 High Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny72mU_dNWMR"
      },
      "source": [
        "# instantiate a model and fit it with a learning rate value of 0.75\n",
        "model = create_model(lr=0.75)\n",
        "lr_high = model.fit(X_train, y_train, batch_size=32, validation_data=(X_test, y_test), epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAqDmTVBNSMR"
      },
      "source": [
        "### 3.2.3 Low Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ech1ER64NXBn",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# instantiate a model and fit it with a learning rate value of 0.0001\n",
        "model = create_model(lr=0.0001)\n",
        "lr_low = model.fit(X_train, y_train, batch_size=32, validation_data=(X_test, y_test), epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZe6DyhANXdU"
      },
      "source": [
        "### 3.2.4 Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn-BdFdMNph-"
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "experiments = []\n",
        "\n",
        "for experiment, label in zip([lr_default, lr_low, lr_high], [\".01_\", \".0001_\", \".75_\"]):\n",
        "\n",
        "    df = pd.DataFrame.from_dict(experiment.history)\n",
        "    df['epoch'] = df.index.values\n",
        "    df['Learning Rate'] = label\n",
        "\n",
        "    experiments.append(df)\n",
        "\n",
        "df = pd.concat(experiments,ignore_index=True)\n",
        "df['Learning Rate'] = df['Learning Rate'].astype('str')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgDX7htLpTlQ"
      },
      "source": [
        "sns.lineplot(x='epoch', y='val_loss', hue='Learning Rate', data=df);\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8GPXqf_qGs9"
      },
      "source": [
        "sns.lineplot(x='epoch', y='val_accuracy', hue='Learning Rate', data=df);\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp1jHQ95swhy"
      },
      "source": [
        "##3.3 Learning Rate Scheduler Callback\n",
        "A great way to tune the learning rate is to use a learning rate scheduler, which adjusts the learning rate at each epoch according to a user-supplied \"schedule\". We'll show how to introduce an exponentially increasing schedule.\n",
        "The idea is you do a trial run with the scheduler, plot the loss vs. epoch, and choose a good learning rate by inspecting the plot. <br>_Rule of thumb: choose a learning rate that is a factor of 10 lower than the learning rate at the minimum of the loss vs. learning rate curve._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6SnRB9Cupwo"
      },
      "source": [
        "Here is a learning rate scheduler that starts the learning rate at 1.e-6 and increases by a factor of ten every 5 epochs. So that over 30 epochs, the learning rate goes from 1.e-6 up to 1.0 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3-iOvmmulKa"
      },
      "source": [
        "def scheduler(epoch):\n",
        "    lr = 1e-6 * 10**(epoch/5)\n",
        "    return lr\n",
        "\n",
        "# Here is the callback\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8i6vHFuvIA9"
      },
      "source": [
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw3qyG1jt7y2"
      },
      "source": [
        "# create the model and fit it, passing in the learning rate scheduler callback that we created\n",
        "epochs = 30\n",
        "batch_size = 32\n",
        "model_for_lr_schedule = create_model()\n",
        "history_for_lr_schedule = model_for_lr_schedule.fit(X_train, y_train, batch_size=batch_size,callbacks=[lr_schedule] , epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swY-13T94ZlS"
      },
      "source": [
        "Since the minimum in the loss vs learning rate curve below is at learning rate of 0.1, we should choose a learning rate of 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BXA30jw1YhV"
      },
      "source": [
        "# plot the result\n",
        "plt.semilogx(history_for_lr_schedule.history[\"lr\"], history_for_lr_schedule.history[\"loss\"])\n",
        "plt.axis([1e-6, 1, 1.0, 2.5])\n",
        "plt.title('learning rate schedule')\n",
        "plt.xlabel('learning rate')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb2aiw_Sgcl7"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to experiment with different learning rates today.\n",
        "\n",
        "---"
      ]
    }
  ]
}